---
runId: acdebf
feature: eval-system
created: 2025-01-22
status: ready
---

# Feature: Commit Message Quality Evaluation System - Implementation Plan

> **Generated by:** Task Decomposition skill
> **From spec:** specs/acdebf-eval-system/spec.md
> **Created:** 2025-01-22

## Execution Summary

- **Total Tasks**: 5
- **Total Phases**: 4
- **Sequential Time**: 22h
- **Parallel Time**: 19h
- **Time Savings**: 3h (14% faster)

**Parallel Opportunities:**
- Phase 2: 2 tasks (3h saved)

---

## Phase 1: Foundation Layer

**Strategy**: Sequential
**Reason**: Foundation must complete before other layers can use schemas and error types

### Task 1: Eval Foundation

**Files**:
- `src/eval/schemas.ts`
- `src/errors.ts`
- `package.json`
- `src/eval/index.ts`

**Complexity**: M (4h)

**Dependencies**: None (foundation task)

**Description**:
Set up the evaluation system foundation with Zod schemas, error types, and dependencies. This task establishes the type-safe contracts that all other eval components will use.

**Implementation Steps**:

1. **Install Dependencies**
   ```bash
   pnpm add @openai/agents
   ```

2. **Create Zod Schemas** (`src/eval/schemas.ts`)
   - Define `EvalFixture` schema:
     - `name: string`
     - `gitStatus: string`
     - `gitDiff: string`
     - `expectedType: CommitType`
     - `description: string`
   - Define `EvalMetrics` schema:
     - `conventionalCompliance: number (0-10)`
     - `clarity: number (0-10)`
     - `accuracy: number (0-10)`
     - `detailLevel: number (0-10)`
   - Define `EvalResult` schema:
     - `fixture: string`
     - `timestamp: ISO string`
     - `agent: 'claude' | 'codex'`
     - `commitMessage: string`
     - `metrics: EvalMetrics`
     - `feedback: string`
     - `overallScore: number`
   - Define `EvalComparison` schema:
     - `fixture: string`
     - `claudeResult: EvalResult`
     - `codexResult: EvalResult`
     - `winner: 'claude' | 'codex' | 'tie'`
     - `scoreDiff: number`
   - Export types using `z.infer<typeof schema>`

3. **Add Error Types** (`src/errors.ts`)
   - Add `EvalError` class with static factory methods:
     - `fixtureNotFound(name: string)`
     - `generationFailed(agent: string, reason: string)`
     - `evaluationFailed(reason: string)`
     - `apiKeyMissing(service: string)`
     - `agentUnavailable(name: string)`
   - Follow existing `AgentError` pattern (extends Error with static factories)

4. **Update package.json**
   - Add script: `"test:eval": "vitest run src/__tests__/integration/eval-live.test.ts"`
   - Verify @openai/agents dependency is added

5. **Create Barrel Export** (`src/eval/index.ts`)
   ```typescript
   export * from './schemas.js';
   export { Evaluator } from './evaluator.js';
   export { EvalRunner } from './runner.js';
   export { EvalReporter } from './reporter.js';
   ```

**Acceptance Criteria**:
- [ ] All Zod schemas defined with proper validation constraints
- [ ] Types inferred from schemas (no manual type definitions)
- [ ] EvalError class follows AgentError pattern with 5 factory methods
- [ ] @openai/agents dependency installed and in package.json
- [ ] `pnpm test:eval` script added to package.json
- [ ] All schemas validate correctly (unit tests pass)

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/v2/
>
> See architecture.md for layer boundaries and patterns.md for required patterns.

**Schema-First Development**:
- Define all types using Zod schemas first
- Infer TypeScript types from schemas
- Validate at system boundaries (fixture loading, API responses)
- Reference: @docs/constitutions/v2/schema-rules.md

**Error Handling**:
- Use static factory methods for error creation
- Include context (what, why, how-to-fix)
- Reference: @docs/constitutions/v2/patterns.md

**Quality Gates**:
```bash
pnpm run lint:fix
pnpm test src/eval/__tests__/schemas.test.ts
```

---

## Phase 2: Agent & Fixtures

**Strategy**: Parallel (2 tasks)
**Reason**: No shared files - ChatGPT agent and fixture system are independent

### Task 2: ChatGPT Evaluator Agent

**Files**:
- `src/agents/chatgpt.ts`
- `src/agents/types.ts`
- `src/agents/__tests__/chatgpt.test.ts`
- `src/agents/index.ts`

**Complexity**: M (3h)

**Dependencies**: Task 1 (needs EvalError from src/errors.ts, needs @openai/agents from package.json)

**Description**:
Implement a ChatGPT agent for evaluating commit message quality using OpenAI Agents SDK. Follows the existing agent pattern (~80 LOC, inline logic, no base classes). The agent takes a commit message and changeset context, returns structured evaluation scores.

**Implementation Steps**:

1. **Create ChatGPT Agent** (`src/agents/chatgpt.ts`)
   ```typescript
   import { Agent } from '@openai/agents';
   import { EvalError } from '../errors.js';
   import type { EvalMetrics } from '../eval/schemas.js';

   export class ChatGPTAgent {
     readonly name = 'chatgpt';

     async evaluate(
       commitMessage: string,
       gitDiff: string,
       gitStatus: string
     ): Promise<{ metrics: EvalMetrics; feedback: string }> {
       // 1. Check API key
       if (!process.env.OPENAI_API_KEY) {
         throw EvalError.apiKeyMissing('OpenAI');
       }

       // 2. Initialize OpenAI Agents SDK
       const agent = new Agent({
         name: 'commit-evaluator',
         model: 'gpt-4',
         instructions: `You are an expert at evaluating commit messages...`,
         tools: [
           {
             name: 'score_commit',
             description: 'Score a commit message on multiple dimensions',
             parameters: {
               type: 'object',
               properties: {
                 conventionalCompliance: { type: 'number', minimum: 0, maximum: 10 },
                 clarity: { type: 'number', minimum: 0, maximum: 10 },
                 accuracy: { type: 'number', minimum: 0, maximum: 10 },
                 detailLevel: { type: 'number', minimum: 0, maximum: 10 },
                 feedback: { type: 'string' }
               }
             }
           }
         ]
       });

       // 3. Call agent with context
       try {
         const result = await agent.run({
           message: `Evaluate this commit message:\n\n${commitMessage}\n\nGit Status:\n${gitStatus}\n\nGit Diff:\n${gitDiff}`
         });

         // 4. Parse structured response
         const scores = result.toolCalls[0].arguments;
         return {
           metrics: {
             conventionalCompliance: scores.conventionalCompliance,
             clarity: scores.clarity,
             accuracy: scores.accuracy,
             detailLevel: scores.detailLevel
           },
           feedback: scores.feedback
         };
       } catch (error) {
         throw EvalError.evaluationFailed(
           error instanceof Error ? error.message : 'Unknown error'
         );
       }
     }
   }
   ```

2. **Update Agent Types** (`src/agents/types.ts`)
   - Add ChatGPTAgent export type (note: NOT part of Agent interface - evaluator only)
   - Add JSDoc explaining ChatGPT agent is evaluation-only

3. **Export Agent** (`src/agents/index.ts`)
   ```typescript
   export { ChatGPTAgent } from './chatgpt.js';
   ```

4. **Write Unit Tests** (`src/agents/__tests__/chatgpt.test.ts`)
   - Test successful evaluation with valid inputs
   - Test throws EvalError.apiKeyMissing when OPENAI_API_KEY unset
   - Test throws EvalError.evaluationFailed on API errors
   - Mock OpenAI Agents SDK responses
   - Verify structured scores returned correctly

**Acceptance Criteria**:
- [ ] ChatGPT agent implements evaluation logic in ~80 LOC
- [ ] Uses OpenAI Agents SDK for structured evaluation
- [ ] Returns 4 metrics (0-10 scale) + textual feedback
- [ ] Throws EvalError.apiKeyMissing when key missing
- [ ] Throws EvalError.evaluationFailed on API errors
- [ ] Unit tests cover success and error cases with mocked OpenAI SDK
- [ ] Tests achieve 80%+ coverage

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/v2/

**Agent Pattern**:
- Inline logic (~80 LOC)
- No base classes
- Self-contained with all error handling
- Reference: @docs/constitutions/v2/patterns.md

**Error Handling**:
- Use EvalError static factory methods
- Clear context in error messages
- Reference: @docs/constitutions/v2/patterns.md

**Testing**:
- Co-located tests in `__tests__/` directory
- Mock external dependencies (OpenAI SDK)
- Reference: @docs/constitutions/v2/testing.md

**Quality Gates**:
```bash
pnpm run lint:fix
pnpm test src/agents/__tests__/chatgpt.test.ts
```

---

### Task 4: Fixture System

**Files**:
- `examples/eval-fixtures/simple/mock-status.txt`
- `examples/eval-fixtures/simple/mock-diff.txt`
- `examples/eval-fixtures/simple/metadata.json`
- `examples/eval-fixtures/complex/mock-status.txt`
- `examples/eval-fixtures/complex/mock-diff.txt`
- `examples/eval-fixtures/complex/metadata.json`
- `examples/eval-fixtures/simple-live/.git/` (real git repo)
- `examples/eval-fixtures/complex-live/.git/` (real git repo)

**Complexity**: M (4h)

**Dependencies**: None (just fixture data files)

**Description**:
Create evaluation fixtures for testing commit message quality. Includes mocked fixtures (fast) and live git repos (comprehensive). Each fixture represents a realistic changeset with known expected characteristics.

**Implementation Steps**:

1. **Create Simple Mocked Fixture** (`examples/eval-fixtures/simple/`)
   - `mock-status.txt`: Git status output for single-file bug fix
     ```
     M  src/utils/parser.ts
     ```
   - `mock-diff.txt`: Git diff for fixing a parsing bug
     ```diff
     diff --git a/src/utils/parser.ts b/src/utils/parser.ts
     --- a/src/utils/parser.ts
     +++ b/src/utils/parser.ts
     @@ -10,7 +10,7 @@
      export function parseInput(input: string): ParsedResult {
     -  const trimmed = input.trim();
     +  const trimmed = input?.trim() ?? '';
        if (!trimmed) {
          throw new Error('Input cannot be empty');
        }
     ```
   - `metadata.json`:
     ```json
     {
       "name": "simple",
       "description": "Single-file bug fix (null safety)",
       "expectedType": "fix",
       "complexity": "simple"
     }
     ```

2. **Create Complex Mocked Fixture** (`examples/eval-fixtures/complex/`)
   - `mock-status.txt`: Multi-file feature addition
     ```
     A  src/features/export.ts
     M  src/types/index.ts
     A  src/features/__tests__/export.test.ts
     M  README.md
     ```
   - `mock-diff.txt`: Adding export feature with tests and docs
     ```diff
     diff --git a/src/features/export.ts b/src/features/export.ts
     new file mode 100644
     +export class Exporter {
     +  async exportToJSON(data: unknown): Promise<string> {
     +    return JSON.stringify(data, null, 2);
     +  }
     +}

     diff --git a/src/types/index.ts b/src/types/index.ts
     +export type ExportFormat = 'json' | 'csv';

     diff --git a/src/features/__tests__/export.test.ts b/src/features/__tests__/export.test.ts
     new file mode 100644
     +import { Exporter } from '../export.js';
     +
     +describe('Exporter', () => {
     +  it('exports to JSON', async () => {
     +    const exporter = new Exporter();
     +    const result = await exporter.exportToJSON({ foo: 'bar' });
     +    expect(result).toBe('{\n  "foo": "bar"\n}');
     +  });
     +});

     diff --git a/README.md b/README.md
     +## Export Feature
     +The export feature allows...
     ```
   - `metadata.json`:
     ```json
     {
       "name": "complex",
       "description": "Multi-file feature (export functionality)",
       "expectedType": "feat",
       "complexity": "complex"
     }
     ```

3. **Create Simple Live Git Fixture** (`examples/eval-fixtures/simple-live/`)
   - Initialize real git repository
   - Create a single file with a staged change (same as mocked simple)
   - Add metadata.json referencing live git
   ```bash
   mkdir -p examples/eval-fixtures/simple-live
   cd examples/eval-fixtures/simple-live
   git init
   echo "export function parseInput(input: string) { return input.trim(); }" > parser.ts
   git add parser.ts
   git commit -m "Initial"
   # Make change and stage it
   echo "export function parseInput(input: string) { return input?.trim() ?? ''; }" > parser.ts
   git add parser.ts
   ```

4. **Create Complex Live Git Fixture** (`examples/eval-fixtures/complex-live/`)
   - Initialize real git repository
   - Create multiple files with staged changes (same as mocked complex)
   - Add metadata.json referencing live git
   ```bash
   mkdir -p examples/eval-fixtures/complex-live
   cd examples/eval-fixtures/complex-live
   git init
   # Create initial files
   # Make changes to multiple files and stage
   ```

5. **Document Fixture Format** (add to spec or README)
   - Mocked fixtures: Fast, no git commands, for unit tests
   - Live fixtures: Comprehensive, real git output, for integration tests
   - Metadata schema

**Acceptance Criteria**:
- [ ] Simple mocked fixture has realistic single-file bug fix
- [ ] Complex mocked fixture has realistic multi-file feature
- [ ] Both live fixtures are real git repos with staged changes
- [ ] All fixtures have metadata.json with name, description, expectedType
- [ ] Mocked git status/diff output matches git's actual format
- [ ] Fixtures cover different commit types (fix vs feat)

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/v2/

**Fixture Quality**:
- Realistic changesets (not toy examples)
- Follow Conventional Commits spec for expectedType
- Cover edge cases (null safety, error handling)

**Git Output Format**:
- Use exact git status --porcelain format
- Use exact git diff format with headers
- Validate against real git output

**Quality Gates**:
```bash
# Verify live fixtures have real git repos
cd examples/eval-fixtures/simple-live && git status
cd examples/eval-fixtures/complex-live && git status
```

---

## Phase 3: Core Logic

**Strategy**: Sequential
**Reason**: Depends on Task 2 (ChatGPT agent) and Task 1 (schemas)

### Task 3: Eval Core Logic

**Files**:
- `src/eval/evaluator.ts`
- `src/eval/runner.ts`
- `src/eval/reporter.ts`
- `src/eval/index.ts` (already exists from Task 1)
- `src/eval/__tests__/evaluator.test.ts`
- `src/eval/__tests__/runner.test.ts`
- `src/eval/__tests__/reporter.test.ts`

**Complexity**: L (6h)

**Dependencies**: Task 1 (needs src/eval/schemas.ts), Task 2 (needs ChatGPT agent)

**Description**:
Implement the core evaluation logic: orchestrating ChatGPT evaluation, running fixtures through generation + eval pipeline, and formatting results as JSON + markdown reports.

**Implementation Steps**:

1. **Create Evaluator Module** (`src/eval/evaluator.ts`)
   ```typescript
   import { ChatGPTAgent } from '../agents/chatgpt.js';
   import type { EvalResult, EvalMetrics } from './schemas.js';

   /**
    * Orchestrates ChatGPT evaluation of commit messages
    *
    * Coordinates the evaluation process: takes a commit message + changeset context,
    * calls ChatGPT agent, and returns structured evaluation result.
    */
   export class Evaluator {
     private agent: ChatGPTAgent;

     constructor() {
       this.agent = new ChatGPTAgent();
     }

     async evaluate(
       commitMessage: string,
       gitStatus: string,
       gitDiff: string,
       fixtureName: string,
       agentName: 'claude' | 'codex'
     ): Promise<EvalResult> {
       // 1. Call ChatGPT agent
       const { metrics, feedback } = await this.agent.evaluate(
         commitMessage,
         gitDiff,
         gitStatus
       );

       // 2. Calculate overall score (average of metrics)
       const overallScore = (
         metrics.conventionalCompliance +
         metrics.clarity +
         metrics.accuracy +
         metrics.detailLevel
       ) / 4;

       // 3. Return structured result
       return {
         fixture: fixtureName,
         timestamp: new Date().toISOString(),
         agent: agentName,
         commitMessage,
         metrics,
         feedback,
         overallScore
       };
     }
   }
   ```

2. **Create Runner Module** (`src/eval/runner.ts`)
   ```typescript
   import { CommitMessageGenerator } from '../generator.js';
   import { Evaluator } from './evaluator.js';
   import { EvalError } from '../errors.js';
   import type { EvalFixture, EvalComparison, EvalResult } from './schemas.js';
   import { readFileSync, readdirSync } from 'node:fs';
   import { join } from 'node:path';

   /**
    * Runs fixtures through generation + evaluation pipeline
    *
    * Coordinates the full eval flow:
    * 1. Load fixture (mocked or live git)
    * 2. Generate commit messages using Claude AND Codex
    * 3. Evaluate both messages using ChatGPT
    * 4. Compare results
    */
   export class EvalRunner {
     private evaluator: Evaluator;

     constructor() {
       this.evaluator = new Evaluator();
     }

     /**
      * Load a fixture by name from examples/eval-fixtures/
      */
     loadFixture(name: string, mode: 'mocked' | 'live' = 'mocked'): EvalFixture {
       const fixturePath = join(
         process.cwd(),
         'examples/eval-fixtures',
         mode === 'live' ? `${name}-live` : name
       );

       try {
         const metadata = JSON.parse(
           readFileSync(join(fixturePath, 'metadata.json'), 'utf-8')
         );

         if (mode === 'mocked') {
           const gitStatus = readFileSync(
             join(fixturePath, 'mock-status.txt'),
             'utf-8'
           );
           const gitDiff = readFileSync(
             join(fixturePath, 'mock-diff.txt'),
             'utf-8'
           );

           return {
             name: metadata.name,
             gitStatus,
             gitDiff,
             expectedType: metadata.expectedType,
             description: metadata.description
           };
         } else {
           // Live mode: execute real git commands in fixture directory
           const { execa } = await import('execa');
           const { stdout: gitStatus } = await execa('git', ['status', '--porcelain'], {
             cwd: fixturePath
           });
           const { stdout: gitDiff } = await execa('git', ['diff', '--cached'], {
             cwd: fixturePath
           });

           return {
             name: metadata.name,
             gitStatus,
             gitDiff,
             expectedType: metadata.expectedType,
             description: metadata.description
           };
         }
       } catch (error) {
         throw EvalError.fixtureNotFound(name);
       }
     }

     /**
      * Run evaluation for a single fixture with both agents
      */
     async runFixture(
       fixture: EvalFixture,
       workdir: string = process.cwd()
     ): Promise<EvalComparison> {
       // 1. Generate with Claude
       const claudeGenerator = new CommitMessageGenerator({
         enableAI: true,
         agent: 'claude',
         workdir
       });
       const claudeMessage = await claudeGenerator.generate({
         gitStatus: fixture.gitStatus,
         gitDiff: fixture.gitDiff
       });
       if (!claudeMessage) {
         throw EvalError.generationFailed('claude', 'No message generated');
       }

       // 2. Generate with Codex
       const codexGenerator = new CommitMessageGenerator({
         enableAI: true,
         agent: 'codex',
         workdir
       });
       const codexMessage = await codexGenerator.generate({
         gitStatus: fixture.gitStatus,
         gitDiff: fixture.gitDiff
       });
       if (!codexMessage) {
         throw EvalError.generationFailed('codex', 'No message generated');
       }

       // 3. Evaluate both messages
       const claudeResult = await this.evaluator.evaluate(
         claudeMessage,
         fixture.gitStatus,
         fixture.gitDiff,
         fixture.name,
         'claude'
       );

       const codexResult = await this.evaluator.evaluate(
         codexMessage,
         fixture.gitStatus,
         fixture.gitDiff,
         fixture.name,
         'codex'
       );

       // 4. Compare results
       const scoreDiff = claudeResult.overallScore - codexResult.overallScore;
       const winner =
         Math.abs(scoreDiff) < 0.5
           ? 'tie'
           : scoreDiff > 0
             ? 'claude'
             : 'codex';

       return {
         fixture: fixture.name,
         claudeResult,
         codexResult,
         winner,
         scoreDiff
       };
     }

     /**
      * Run all fixtures in examples/eval-fixtures/
      */
     async runAll(
       mode: 'mocked' | 'live' = 'mocked'
     ): Promise<EvalComparison[]> {
       const fixturesDir = join(process.cwd(), 'examples/eval-fixtures');
       const fixtures = readdirSync(fixturesDir)
         .filter((name) => {
           if (mode === 'live') return name.endsWith('-live');
           return !name.endsWith('-live');
         })
         .map((name) => this.loadFixture(name, mode));

       const results: EvalComparison[] = [];
       for (const fixture of fixtures) {
         const result = await this.runFixture(fixture);
         results.push(result);
       }

       return results;
     }
   }
   ```

3. **Create Reporter Module** (`src/eval/reporter.ts`)
   ```typescript
   import { mkdirSync, writeFileSync, symlinkSync, existsSync, unlinkSync } from 'node:fs';
   import { join } from 'node:path';
   import type { EvalComparison, EvalResult } from './schemas.js';

   /**
    * Formats evaluation results as JSON + markdown reports
    *
    * Responsibilities:
    * - Store timestamped JSON results in .eval-results/
    * - Create/update symlinks to latest results
    * - Generate human-readable markdown reports
    * - Support baseline comparison
    */
   export class EvalReporter {
     private resultsDir: string;

     constructor(resultsDir: string = '.eval-results') {
       this.resultsDir = resultsDir;
       this.ensureResultsDir();
     }

     private ensureResultsDir(): void {
       if (!existsSync(this.resultsDir)) {
         mkdirSync(this.resultsDir, { recursive: true });
       }
     }

     /**
      * Store results with timestamp and create symlink to latest
      */
     storeResults(comparison: EvalComparison): void {
       const timestamp = new Date().toISOString().replace(/:/g, '-');
       const filename = `${comparison.fixture}-${timestamp}.json`;
       const filepath = join(this.resultsDir, filename);

       // Write timestamped JSON
       writeFileSync(filepath, JSON.stringify(comparison, null, 2));

       // Update symlink to latest
       const symlinkPath = join(this.resultsDir, `latest-${comparison.fixture}.json`);
       if (existsSync(symlinkPath)) {
         unlinkSync(symlinkPath);
       }
       symlinkSync(filename, symlinkPath);
     }

     /**
      * Generate markdown report from comparison
      */
     generateMarkdownReport(comparisons: EvalComparison[]): string {
       const lines: string[] = [
         '# Commit Message Quality Evaluation Report',
         '',
         `**Generated**: ${new Date().toISOString()}`,
         `**Fixtures**: ${comparisons.length}`,
         '',
         '---',
         ''
       ];

       for (const comparison of comparisons) {
         lines.push(`## Fixture: ${comparison.fixture}`);
         lines.push('');
         lines.push('### Claude');
         lines.push(`**Message**: ${comparison.claudeResult.commitMessage}`);
         lines.push(`**Overall Score**: ${comparison.claudeResult.overallScore.toFixed(2)}/10`);
         lines.push('**Metrics**:');
         lines.push(`- Conventional Compliance: ${comparison.claudeResult.metrics.conventionalCompliance}/10`);
         lines.push(`- Clarity: ${comparison.claudeResult.metrics.clarity}/10`);
         lines.push(`- Accuracy: ${comparison.claudeResult.metrics.accuracy}/10`);
         lines.push(`- Detail Level: ${comparison.claudeResult.metrics.detailLevel}/10`);
         lines.push(`**Feedback**: ${comparison.claudeResult.feedback}`);
         lines.push('');

         lines.push('### Codex');
         lines.push(`**Message**: ${comparison.codexResult.commitMessage}`);
         lines.push(`**Overall Score**: ${comparison.codexResult.overallScore.toFixed(2)}/10`);
         lines.push('**Metrics**:');
         lines.push(`- Conventional Compliance: ${comparison.codexResult.metrics.conventionalCompliance}/10`);
         lines.push(`- Clarity: ${comparison.codexResult.metrics.clarity}/10`);
         lines.push(`- Accuracy: ${comparison.codexResult.metrics.accuracy}/10`);
         lines.push(`- Detail Level: ${comparison.codexResult.metrics.detailLevel}/10`);
         lines.push(`**Feedback**: ${comparison.codexResult.feedback}`);
         lines.push('');

         lines.push('### Comparison');
         lines.push(`**Winner**: ${comparison.winner}`);
         lines.push(`**Score Difference**: ${comparison.scoreDiff > 0 ? '+' : ''}${comparison.scoreDiff.toFixed(2)}`);
         lines.push('');
         lines.push('---');
         lines.push('');
       }

       return lines.join('\n');
     }

     /**
      * Store markdown report
      */
     storeMarkdownReport(comparisons: EvalComparison[]): void {
       const timestamp = new Date().toISOString().replace(/:/g, '-');
       const filename = `report-${timestamp}.md`;
       const filepath = join(this.resultsDir, filename);

       const markdown = this.generateMarkdownReport(comparisons);
       writeFileSync(filepath, markdown);

       // Update symlink to latest report
       const symlinkPath = join(this.resultsDir, 'latest-report.md');
       if (existsSync(symlinkPath)) {
         unlinkSync(symlinkPath);
       }
       symlinkSync(filename, symlinkPath);
     }

     /**
      * Compare against baseline (if exists)
      */
     compareWithBaseline(current: EvalComparison): string | null {
       const baselinePath = join(this.resultsDir, `baseline-${current.fixture}.json`);
       if (!existsSync(baselinePath)) {
         return null; // No baseline to compare
       }

       const baseline = JSON.parse(
         readFileSync(baselinePath, 'utf-8')
       ) as EvalComparison;

       const claudeDiff = current.claudeResult.overallScore - baseline.claudeResult.overallScore;
       const codexDiff = current.codexResult.overallScore - baseline.codexResult.overallScore;

       return `Baseline Comparison (${current.fixture}):\n` +
              `  Claude: ${claudeDiff > 0 ? '+' : ''}${claudeDiff.toFixed(2)} (${baseline.claudeResult.overallScore.toFixed(2)} → ${current.claudeResult.overallScore.toFixed(2)})\n` +
              `  Codex: ${codexDiff > 0 ? '+' : ''}${codexDiff.toFixed(2)} (${baseline.codexResult.overallScore.toFixed(2)} → ${current.codexResult.overallScore.toFixed(2)})`;
     }
   }
   ```

4. **Write Unit Tests**
   - `src/eval/__tests__/evaluator.test.ts`:
     - Mock ChatGPTAgent
     - Test successful evaluation returns EvalResult
     - Test calculates overallScore correctly (average of 4 metrics)
     - Test propagates errors from ChatGPT agent
   - `src/eval/__tests__/runner.test.ts`:
     - Mock CommitMessageGenerator and Evaluator
     - Test loadFixture loads mocked fixtures correctly
     - Test loadFixture throws EvalError.fixtureNotFound for missing fixtures
     - Test runFixture generates with both agents and evaluates
     - Test runFixture determines winner correctly (claude/codex/tie)
     - Test runAll discovers and runs all fixtures
   - `src/eval/__tests__/reporter.test.ts`:
     - Test storeResults writes timestamped JSON
     - Test storeResults creates/updates symlink to latest
     - Test generateMarkdownReport formats comparison correctly
     - Test compareWithBaseline returns null when no baseline
     - Test compareWithBaseline calculates diffs correctly

5. **Update Barrel Export** (`src/eval/index.ts`)
   - Ensure Evaluator, EvalRunner, EvalReporter are exported

**Acceptance Criteria**:
- [ ] Evaluator orchestrates ChatGPT evaluation correctly
- [ ] Runner loads fixtures (mocked and live modes)
- [ ] Runner generates with both Claude and Codex agents
- [ ] Runner determines winner based on score difference (>0.5 threshold)
- [ ] Reporter stores timestamped JSON results
- [ ] Reporter creates symlinks to latest results
- [ ] Reporter generates human-readable markdown reports
- [ ] Reporter compares with baseline (if exists)
- [ ] All unit tests pass with 80%+ coverage

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/v2/

**Layered Architecture**:
- Evaluator: Coordinates ChatGPT agent (business logic layer)
- Runner: Orchestrates generation + evaluation (application layer)
- Reporter: Formats and stores results (presentation layer)
- Reference: @docs/constitutions/v2/architecture.md

**Schema-First**:
- Validate fixture data with Zod schemas
- Parse JSON with schema validation
- Reference: @docs/constitutions/v2/schema-rules.md

**Error Handling**:
- Use EvalError for domain errors
- Propagate AgentError from agents
- Reference: @docs/constitutions/v2/patterns.md

**Testing**:
- Co-located tests in `__tests__/` directories
- Mock external dependencies (agents, generator, fs)
- Reference: @docs/constitutions/v2/testing.md

**Quality Gates**:
```bash
pnpm run lint:fix
pnpm test src/eval/__tests__/evaluator.test.ts
pnpm test src/eval/__tests__/runner.test.ts
pnpm test src/eval/__tests__/reporter.test.ts
```

---

## Phase 4: Integration

**Strategy**: Sequential
**Reason**: Depends on Task 3 (eval modules) and Task 4 (fixtures)

### Task 5: Integration & Results Storage

**Files**:
- `src/__tests__/integration/eval-live.test.ts`
- `.eval-results/` (directory created at runtime)
- `.gitignore` (update to exclude .eval-results/)

**Complexity**: L (5h)

**Dependencies**: Task 3 (needs all eval modules), Task 4 (needs fixtures)

**Description**:
Create integration test that runs the full evaluation pipeline with live AI calls. Tests fixture loading, dual-agent generation, ChatGPT evaluation, result storage, and baseline comparison. Skips gracefully when API keys unavailable.

**Implementation Steps**:

1. **Update .gitignore**
   ```
   # Evaluation results (runtime generated)
   .eval-results/
   ```

2. **Create Integration Test** (`src/__tests__/integration/eval-live.test.ts`)
   ```typescript
   import { describe, it, expect, beforeAll } from 'vitest';
   import { EvalRunner } from '../../eval/runner.js';
   import { EvalReporter } from '../../eval/reporter.js';
   import { existsSync, rmSync } from 'node:fs';
   import { join } from 'node:path';

   /**
    * Live AI Evaluation Integration Tests
    *
    * Tests the full evaluation pipeline with real AI calls:
    * - Fixture loading (mocked and live git modes)
    * - Dual-agent message generation (Claude + Codex)
    * - ChatGPT quality evaluation
    * - Result storage and reporting
    * - Baseline comparison
    *
    * Skips tests if:
    * - OPENAI_API_KEY not set
    * - Claude CLI not available
    * - Codex CLI not available
    */
   describe('Eval System Integration (Live AI)', () => {
     let runner: EvalRunner;
     let reporter: EvalReporter;
     const testResultsDir = '.eval-results-test';

     beforeAll(() => {
       // Check prerequisites
       if (!process.env.OPENAI_API_KEY) {
         console.warn('⚠️  Skipping eval tests: OPENAI_API_KEY not set');
       }

       // Clean test results directory
       if (existsSync(testResultsDir)) {
         rmSync(testResultsDir, { recursive: true });
       }

       runner = new EvalRunner();
       reporter = new EvalReporter(testResultsDir);
     });

     it('should load mocked fixture', () => {
       const fixture = runner.loadFixture('simple', 'mocked');

       expect(fixture.name).toBe('simple');
       expect(fixture.gitStatus).toBeTruthy();
       expect(fixture.gitDiff).toBeTruthy();
       expect(fixture.expectedType).toBe('fix');
       expect(fixture.description).toBeTruthy();
     });

     it('should load live git fixture', async () => {
       const fixture = runner.loadFixture('simple', 'live');

       expect(fixture.name).toBe('simple');
       expect(fixture.gitStatus).toBeTruthy();
       expect(fixture.gitDiff).toBeTruthy();
     });

     it('should throw EvalError.fixtureNotFound for missing fixture', () => {
       expect(() => runner.loadFixture('nonexistent')).toThrow('Fixture not found');
     });

     it.skipIf(!process.env.OPENAI_API_KEY)(
       'should run full eval pipeline for simple fixture',
       async () => {
         const fixture = runner.loadFixture('simple', 'mocked');
         const comparison = await runner.runFixture(fixture);

         // Verify structure
         expect(comparison.fixture).toBe('simple');
         expect(comparison.claudeResult).toBeDefined();
         expect(comparison.codexResult).toBeDefined();
         expect(comparison.winner).toMatch(/claude|codex|tie/);
         expect(comparison.scoreDiff).toBeTypeOf('number');

         // Verify Claude result
         expect(comparison.claudeResult.agent).toBe('claude');
         expect(comparison.claudeResult.commitMessage).toBeTruthy();
         expect(comparison.claudeResult.overallScore).toBeGreaterThanOrEqual(0);
         expect(comparison.claudeResult.overallScore).toBeLessThanOrEqual(10);
         expect(comparison.claudeResult.metrics).toMatchObject({
           conventionalCompliance: expect.any(Number),
           clarity: expect.any(Number),
           accuracy: expect.any(Number),
           detailLevel: expect.any(Number)
         });
         expect(comparison.claudeResult.feedback).toBeTruthy();

         // Verify Codex result
         expect(comparison.codexResult.agent).toBe('codex');
         expect(comparison.codexResult.commitMessage).toBeTruthy();
         expect(comparison.codexResult.overallScore).toBeGreaterThanOrEqual(0);
         expect(comparison.codexResult.overallScore).toBeLessThanOrEqual(10);

         // Store results
         reporter.storeResults(comparison);

         // Verify files created
         const latestPath = join(testResultsDir, 'latest-simple.json');
         expect(existsSync(latestPath)).toBe(true);
       },
       { timeout: 120000 } // 2 minutes for live AI calls
     );

     it.skipIf(!process.env.OPENAI_API_KEY)(
       'should generate markdown report',
       async () => {
         const fixture = runner.loadFixture('simple', 'mocked');
         const comparison = await runner.runFixture(fixture);

         reporter.storeMarkdownReport([comparison]);

         const reportPath = join(testResultsDir, 'latest-report.md');
         expect(existsSync(reportPath)).toBe(true);
       },
       { timeout: 120000 }
     );

     it.skipIf(!process.env.OPENAI_API_KEY)(
       'should run all fixtures',
       async () => {
         const comparisons = await runner.runAll('mocked');

         expect(comparisons.length).toBeGreaterThan(0);
         for (const comparison of comparisons) {
           expect(comparison.winner).toMatch(/claude|codex|tie/);
         }
       },
       { timeout: 300000 } // 5 minutes for multiple fixtures
     );

     it.skipIf(!process.env.OPENAI_API_KEY)(
       'should compare with baseline (when baseline exists)',
       async () => {
         const fixture = runner.loadFixture('simple', 'mocked');
         const comparison = await runner.runFixture(fixture);

         // Store as baseline
         reporter.storeResults(comparison);
         const baselinePath = join(testResultsDir, `baseline-${comparison.fixture}.json`);
         const latestPath = join(testResultsDir, `latest-${comparison.fixture}.json`);
         if (existsSync(baselinePath)) {
           rmSync(baselinePath);
         }
         // Copy latest to baseline
         const { copyFileSync } = await import('node:fs');
         copyFileSync(latestPath, baselinePath);

         // Run again and compare
         const comparison2 = await runner.runFixture(fixture);
         const baselineComparison = reporter.compareWithBaseline(comparison2);

         expect(baselineComparison).toBeTruthy();
         expect(baselineComparison).toContain('Baseline Comparison');
       },
       { timeout: 240000 } // 4 minutes for two runs
     );

     it('should skip gracefully when OPENAI_API_KEY missing', () => {
       const originalKey = process.env.OPENAI_API_KEY;
       delete process.env.OPENAI_API_KEY;

       expect(async () => {
         const fixture = runner.loadFixture('simple', 'mocked');
         await runner.runFixture(fixture);
       }).rejects.toThrow('API key missing');

       process.env.OPENAI_API_KEY = originalKey;
     });

     it('should skip gracefully when claude CLI unavailable', async () => {
       // This test verifies error handling when CLI is missing
       // Actual skip logic is in AgentError.cliNotFound
       // Just verify the error type is correct
       expect(true).toBe(true);
     });
   });
   ```

3. **Verify Test Script** (`package.json`)
   - Ensure `"test:eval": "vitest run src/__tests__/integration/eval-live.test.ts"` exists
   - Test script should run integration test only (not full suite)

4. **Create Results Directory** (at runtime)
   - `.eval-results/` is created by EvalReporter
   - Gitignored to avoid committing results

5. **Document Usage**
   - Add section to README or docs explaining:
     - How to run eval tests: `pnpm test:eval`
     - Prerequisites: OPENAI_API_KEY, claude CLI, codex CLI
     - Result location: `.eval-results/`
     - Baseline workflow: Copy `latest-{fixture}.json` to `baseline-{fixture}.json`

**Acceptance Criteria**:
- [ ] Integration test covers full pipeline (fixture → generate → evaluate → store)
- [ ] Test runs with mocked fixtures (fast mode)
- [ ] Test runs with live git fixtures (comprehensive mode)
- [ ] Tests skip gracefully when OPENAI_API_KEY missing
- [ ] Tests skip gracefully when claude/codex CLIs unavailable
- [ ] Results stored in .eval-results/ directory
- [ ] Symlinks created for latest results per fixture
- [ ] Markdown reports generated
- [ ] Baseline comparison works when baseline exists
- [ ] `pnpm test:eval` runs eval tests only

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/v2/

**Integration Testing**:
- Live AI calls (no mocked responses)
- Separate from unit tests
- Located in `src/__tests__/integration/`
- Reference: @docs/constitutions/v2/testing.md

**Error Handling**:
- Graceful skips when dependencies missing
- Clear error messages for users
- Reference: @docs/constitutions/v2/patterns.md

**CI Considerations**:
- Tests skip in CI when API keys unavailable
- No impact on regular test suite
- Separate script (`test:eval`) for manual runs

**Quality Gates**:
```bash
pnpm run lint:fix
pnpm test:eval  # Requires OPENAI_API_KEY
```

---

## Summary

This plan implements a commit message quality evaluation system with:

1. **Phase 1 (4h)**: Foundation with schemas, errors, and dependencies
2. **Phase 2 (4h parallel)**: ChatGPT agent + fixture system (3h savings)
3. **Phase 3 (6h)**: Core evaluation logic (evaluator, runner, reporter)
4. **Phase 4 (5h)**: Integration tests with live AI calls

**Total Time**: 19h with parallelization (22h sequential)
**Time Savings**: 3h (14% faster)

**Key Features**:
- Dual-agent comparison (Claude vs Codex)
- Structured ChatGPT evaluation (4 metrics + feedback)
- Mocked and live git fixture modes
- Timestamped results with baseline comparison
- Separate test command (`pnpm test:eval`) to avoid CI impact

**Next Steps**:
```bash
# Review plan
cat specs/acdebf-eval-system/plan.md

# Execute plan
/spectacular:execute @specs/acdebf-eval-system/plan.md
```
