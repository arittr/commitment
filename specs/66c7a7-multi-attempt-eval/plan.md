---
runId: 66c7a7
feature: multi-attempt-eval
created: 2025-10-23
status: ready
---

# Feature: Multi-Attempt Evaluation System - Implementation Plan

> **Generated by:** Task Decomposition skill
> **From spec:** specs/66c7a7-multi-attempt-eval/spec.md
> **Created:** 2025-10-23

## Execution Summary

- **Total Tasks**: 6
- **Total Phases**: 4
- **Sequential Time**: 25h
- **Parallel Time**: 19h
- **Time Savings**: 6h (24%)

**Parallel Opportunities:**

- Phase 2: 3 tasks (6h saved)

---

## Phase 1: Foundation Layer

**Strategy**: Sequential
**Reason**: Core foundation must be established first - all other tasks depend on schemas, types, and errors.

### Task 1: Core Foundation - Schemas, Types, and Errors

**Files**:
- `src/eval/core/schemas.ts`
- `src/eval/core/types.ts`
- `src/eval/core/errors.ts`
- `src/eval/core/index.ts`

**Complexity**: M (4h)

**Dependencies**: None (foundation layer)

**Description**:
Create the foundational type system for the multi-attempt evaluation system. This includes all Zod schemas with discriminated unions for attempt outcomes, type inference and exports, and a comprehensive EvaluationError class with factory methods for different error scenarios.

**Implementation Steps**:

1. Create `src/eval/core/schemas.ts`:
   - Define `attemptOutcomeSchema` as discriminated union (status: "success" | "failure")
   - Success branch: commitMessage, metrics, overallScore, attemptNumber
   - Failure branch: failureType enum (cleaning, validation, generation, api_error), failureReason, attemptNumber
   - Define `evalResultSchema` with attempts array (length = 3), finalScore, consistencyScore, errorRateImpact, successRate, reasoning, bestAttempt
   - Define `metaEvaluationInputSchema` for ChatGPT input
   - Define `metaEvaluationOutputSchema` for ChatGPT output with outputType pattern
   - Define `evalComparisonSchema` with both agent results and winner
   - Add JSDoc comments for all schemas
   - Follow schema-rules.md (validation, error messages, defaults)

2. Create `src/eval/core/types.ts`:
   - Export inferred types: `AttemptOutcome`, `EvalResult`, `MetaEvaluation`, `EvalComparison`
   - Add type guards if needed (e.g., `isSuccessOutcome`, `isFailureOutcome`)
   - Re-export from `z.infer<typeof schema>`

3. Create `src/eval/core/errors.ts`:
   - Define `EvaluationError` class extending Error
   - Add factory methods: `metaEvaluationFailed()`, `invalidAttemptCount()`, `missingFixture()`
   - Follow error pattern from patterns.md (what, why, how-to-fix)
   - Include error codes for categorization

4. Create `src/eval/core/index.ts`:
   - Export all schemas from schemas.ts
   - Export all types from types.ts
   - Export EvaluationError from errors.ts
   - Keep barrel exports clean and organized

5. Add co-located tests in `src/eval/core/__tests__/`:
   - `schemas.test.ts` - Test all schemas with valid/invalid inputs
   - `errors.test.ts` - Test error factory methods and messages
   - Aim for 80%+ coverage

**Acceptance Criteria**:
- [ ] All schemas defined with proper validation and error messages
- [ ] Types correctly inferred from schemas via `z.infer<typeof schema>`
- [ ] Discriminated union works correctly (TypeScript narrows on `status` field)
- [ ] EvaluationError class has factory methods for all error scenarios
- [ ] Error messages follow "what, why, how-to-fix" pattern
- [ ] All schemas follow schema-rules.md (validation at boundaries, defaults, transforms)
- [ ] Tests pass with 80%+ coverage
- [ ] Linting passes: `bun run lint`
- [ ] Type checking passes: `bun run type-check`

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

- Schema-first development (schema-rules.md)
- Error handling patterns (patterns.md)
- Discriminated unions for type safety
- Factory methods for error creation

**TDD**: Follow `test-driven-development` skill
- Write schema tests first
- Watch them fail (invalid schema)
- Implement schema
- Watch tests pass

**Quality Gates**:
```bash
bun run lint
bun run type-check
bun test src/eval/core/__tests__/
```

---

## Phase 2: Independent Layers (Parallel)

**Strategy**: Parallel
**Reason**: All three tasks only depend on Core Foundation. They have no file overlaps and can be developed simultaneously.

### Task 2: Utility Layer - Error Categorization and Best Attempt Selection

**Files**:
- `src/eval/utils/error-categorization.ts`
- `src/eval/utils/best-attempt.ts`
- `src/eval/utils/index.ts`

**Complexity**: S (2h)

**Dependencies**: Task 1 (Core Foundation - uses types and errors)

**Description**:
Create pure utility functions for error categorization and best attempt selection. These are stateless, side-effect-free functions that support the evaluation pipeline.

**Implementation Steps**:

1. Create `src/eval/utils/error-categorization.ts`:
   - Define `categorizeError(error: unknown): FailureType` function
   - Pattern-based detection:
     - ENOENT → "api_error"
     - "Invalid conventional commit" → "validation"
     - Thinking/COT artifacts → "cleaning"
     - Agent timeout/failure → "generation"
   - Return failure type enum
   - Add comprehensive JSDoc with examples
   - Pure function - no side effects

2. Create `src/eval/utils/best-attempt.ts`:
   - Define `getBestAttempt(attempts: AttemptOutcome[]): AttemptOutcome | undefined` function
   - Find highest-scoring successful attempt
   - Return undefined if all attempts failed
   - Handle edge cases (empty array, no successes)
   - Add comprehensive JSDoc with examples
   - Pure function - no side effects

3. Create `src/eval/utils/index.ts`:
   - Export categorizeError from error-categorization.ts
   - Export getBestAttempt from best-attempt.ts
   - Clean barrel exports

4. Add co-located tests in `src/eval/utils/__tests__/`:
   - `error-categorization.test.ts` - Test all error patterns
   - `best-attempt.test.ts` - Test selection logic with various scenarios
   - Test edge cases (empty arrays, all failures, ties)
   - Aim for 80%+ coverage

**Acceptance Criteria**:
- [ ] Error categorization correctly identifies all 4 failure types
- [ ] Pattern matching works for ENOENT, validation errors, cleaning artifacts, generation failures
- [ ] Best attempt selection returns highest-scoring successful attempt
- [ ] Edge cases handled (empty, all failures, no successes)
- [ ] Functions are pure (no side effects, same input → same output)
- [ ] Comprehensive JSDoc with examples
- [ ] Tests pass with 80%+ coverage
- [ ] Linting passes: `bun run lint`
- [ ] Type checking passes: `bun run type-check`

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

- Pure functions (no state, no side effects - patterns.md)
- Pattern matching for error detection
- Type guards where appropriate

**TDD**: Follow `test-driven-development` skill
- Write tests for all error patterns first
- Watch them fail
- Implement categorization logic
- Watch tests pass

**Quality Gates**:
```bash
bun run lint
bun run type-check
bun test src/eval/utils/__tests__/
```

---

### Task 3: Evaluator Layer - Single-Attempt and Meta-Evaluation

**Files**:
- `src/eval/evaluators/single-attempt.ts`
- `src/eval/evaluators/meta-evaluator.ts`
- `src/eval/evaluators/chatgpt-agent.ts`
- `src/eval/evaluators/index.ts`

**Complexity**: L (6h)

**Dependencies**: Task 1 (Core Foundation - uses schemas, types, errors)

**Description**:
Create the complete evaluation subsystem with ChatGPT integration. This includes single-attempt scoring (renamed from evaluator.ts), meta-evaluation across 3 attempts, and a shared ChatGPT agent wrapper using OpenAI Agents SDK.

**Implementation Steps**:

1. Create `src/eval/evaluators/chatgpt-agent.ts`:
   - Import OpenAI Agents SDK: `import { Agent, run } from '@openai/agents'`
   - Create `ChatGPTAgent` class with generic `evaluate<T>()` method
   - Use `gpt-5` model (per tech-stack.md)
   - Use `outputType` pattern with Zod schema (NOT tools)
   - Access via `result.finalOutput` (NOT `result.toolCalls`)
   - Handle errors and throw EvaluationError
   - Add timeout handling
   - Comprehensive JSDoc with OpenAI SDK usage examples

2. Create `src/eval/evaluators/single-attempt.ts` (rename from evaluator.ts):
   - Import ChatGPTAgent
   - Define `SingleAttemptEvaluator` class
   - `evaluate(commitMessage, diff, fixture)` method
   - Use ChatGPTAgent with single-attempt schema
   - Calculate metrics: clarity, specificity, conventional format, scope
   - Return evaluation with overallScore
   - Handle errors gracefully
   - Follow existing evaluator patterns but updated for new schemas

3. Create `src/eval/evaluators/meta-evaluator.ts`:
   - Import ChatGPTAgent
   - Define `MetaEvaluator` class
   - `evaluate(attempts: AttemptOutcome[], fixture)` method
   - Build prompt with all 3 attempts (successes and failures)
   - Use ChatGPTAgent with metaEvaluationOutputSchema
   - Calculate: finalScore, consistencyScore, errorRateImpact, successRate
   - Provide reasoning even if 0/3 attempts succeeded
   - Penalize failures (2/3 success ≠ average of 2 scores)
   - Handle all-failure case gracefully
   - Comprehensive error handling

4. Create `src/eval/evaluators/index.ts`:
   - Export ChatGPTAgent from chatgpt-agent.ts
   - Export SingleAttemptEvaluator from single-attempt.ts
   - Export MetaEvaluator from meta-evaluator.ts
   - Clean barrel exports

5. Add co-located tests in `src/eval/evaluators/__tests__/`:
   - `chatgpt-agent.test.ts` - Mock OpenAI SDK, test evaluation flow
   - `single-attempt.test.ts` - Test single-attempt scoring logic
   - `meta-evaluator.test.ts` - Test meta-evaluation with various scenarios (3/3, 2/3, 1/3, 0/3)
   - Test error handling and edge cases
   - Aim for 80%+ coverage

**Acceptance Criteria**:
- [ ] ChatGPTAgent uses `gpt-5` model and `outputType` pattern
- [ ] ChatGPTAgent accesses data via `result.finalOutput` (NOT `result.toolCalls`)
- [ ] SingleAttemptEvaluator correctly scores individual attempts
- [ ] MetaEvaluator evaluates all 3 attempts together
- [ ] Meta-evaluation calculates finalScore, consistencyScore, errorRateImpact, successRate
- [ ] Meta-evaluation provides reasoning even for 0/3 success
- [ ] Failures are penalized (2/3 ≠ average of 2)
- [ ] Error handling follows EvaluationError patterns
- [ ] OpenAI SDK integration follows tech-stack.md requirements
- [ ] Tests pass with 80%+ coverage (mocked OpenAI calls)
- [ ] Linting passes: `bun run lint`
- [ ] Type checking passes: `bun run type-check`

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

- OpenAI Agents SDK usage (tech-stack.md):
  - Model: `gpt-5`
  - Pattern: `outputType` with Zod schema
  - Access: `result.finalOutput`
- Error handling with EvaluationError (patterns.md)
- Schema validation at boundaries (schema-rules.md)

**TDD**: Follow `test-driven-development` skill
- Write tests with mocked OpenAI SDK first
- Test various attempt combinations (3/3, 2/3, 1/3, 0/3)
- Watch tests fail
- Implement evaluators
- Watch tests pass

**Quality Gates**:
```bash
bun run lint
bun run type-check
bun test src/eval/evaluators/__tests__/
```

---

### Task 5: Reporter Layer - Markdown, JSON, and CLI Reporting

**Files**:
- `src/eval/reporters/markdown-reporter.ts`
- `src/eval/reporters/json-reporter.ts`
- `src/eval/reporters/cli-reporter.ts`
- `src/eval/reporters/index.ts`

**Complexity**: M (4h)

**Dependencies**: Task 1 (Core Foundation - uses types)

**Description**:
Create the complete reporting subsystem with three reporters: markdown for human-readable reports, JSON for structured storage, and CLI for real-time progress indicators.

**Implementation Steps**:

1. Create `src/eval/reporters/markdown-reporter.ts`:
   - Define `MarkdownReporter` class
   - `generateReport(results: EvalComparison)` method
   - Show per-attempt details (attempt number, status, score OR failure type + reason)
   - Display meta-evaluation with finalScore, consistency, error rate, reasoning
   - Show success rate breakdown (3/3, 2/3, 1/3, 0/3)
   - Compare agents using finalScore
   - Format using markdown tables and sections
   - Include best attempt details
   - Write to `src/eval/results/latest-report.md`

2. Create `src/eval/reporters/json-reporter.ts`:
   - Define `JSONReporter` class
   - `saveResults(results: EvalResult, fixture: string)` method
   - Generate timestamped filename: `{fixture}-{timestamp}.json`
   - Save to `src/eval/results/{filename}`
   - Create/update symlink: `latest-{fixture}.json` → timestamped file
   - Validate JSON structure before saving
   - Handle file system errors gracefully

3. Create `src/eval/reporters/cli-reporter.ts`:
   - Define `CLIReporter` class
   - `reportAttemptStart(attemptNumber: number)` method
   - `reportAttemptSuccess(attemptNumber: number, score: number)` method
   - `reportAttemptFailure(attemptNumber: number, failureType: FailureType)` method
   - `reportSummary(successRate: string, finalScore: number)` method
   - Use chalk for colored output (green for success, red for failure, yellow for warnings)
   - Show progress indicators for each attempt
   - Display summary with success rate and final score

4. Create `src/eval/reporters/index.ts`:
   - Export MarkdownReporter from markdown-reporter.ts
   - Export JSONReporter from json-reporter.ts
   - Export CLIReporter from cli-reporter.ts
   - Clean barrel exports

5. Add co-located tests in `src/eval/reporters/__tests__/`:
   - `markdown-reporter.test.ts` - Test markdown formatting
   - `json-reporter.test.ts` - Test JSON saving and symlinking
   - `cli-reporter.test.ts` - Test CLI output (mock console)
   - Test various scenarios (all success, all failure, mixed)
   - Aim for 80%+ coverage

**Acceptance Criteria**:
- [ ] Markdown report shows per-attempt details with scores or failure info
- [ ] Markdown report displays meta-evaluation with finalScore and reasoning
- [ ] JSON reporter creates timestamped files correctly
- [ ] JSON reporter creates/updates symlinks to latest files
- [ ] CLI reporter shows progress for each attempt with colored output
- [ ] CLI reporter displays summary with success rate and final score
- [ ] All reporters handle edge cases (all failures, missing data)
- [ ] File system operations have proper error handling
- [ ] Tests pass with 80%+ coverage
- [ ] Linting passes: `bun run lint`
- [ ] Type checking passes: `bun run type-check`

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

- Error handling for file operations (patterns.md)
- Type safety with validated inputs (schema-rules.md)
- User-friendly output formatting

**TDD**: Follow `test-driven-development` skill
- Write tests for report formatting first
- Test various result scenarios
- Watch tests fail
- Implement reporters
- Watch tests pass

**Quality Gates**:
```bash
bun run lint
bun run type-check
bun test src/eval/reporters/__tests__/
```

---

## Phase 3: Orchestration Layer

**Strategy**: Sequential
**Reason**: Runner layer depends on Evaluator and Utility layers from Phase 2.

### Task 4: Runner Layer - Attempt Runner and Eval Runner

**Files**:
- `src/eval/runners/attempt-runner.ts`
- `src/eval/runners/eval-runner.ts`
- `src/eval/runners/index.ts`

**Complexity**: L (6h)

**Dependencies**: Task 1 (Core), Task 2 (Utility), Task 3 (Evaluator)

**Description**:
Create the complete orchestration subsystem with two runners: AttemptRunner for the 3-attempt loop with error handling, and EvalRunner for the main pipeline orchestration (fixtures → attempts → meta-eval → comparison).

**Implementation Steps**:

1. Create `src/eval/runners/attempt-runner.ts`:
   - Define `AttemptRunner` class
   - `runAttempts(agent, fixture)` method
   - Loop 3 times (never short-circuit on failure)
   - For each attempt:
     - Call CommitMessageGenerator
     - If success: evaluate with SingleAttemptEvaluator
     - If failure: catch error, categorize with categorizeError(), create failure outcome
   - Return array of 3 AttemptOutcomes
   - Each attempt is independent (no retry logic)
   - Comprehensive error handling with try/catch per attempt
   - Use CLIReporter for progress updates

2. Create `src/eval/runners/eval-runner.ts`:
   - Define `EvalRunner` class
   - `run(fixtures)` method implementing main pipeline:
     1. Load fixtures
     2. For each fixture, for each agent:
        - Call AttemptRunner.runAttempts() → 3 AttemptOutcomes
        - Call MetaEvaluator.evaluate() → EvalResult with finalScore
        - Store results via JSONReporter
     3. Compare agents using finalScore
     4. Generate markdown report via MarkdownReporter
   - Handle top-level errors gracefully
   - Use fallback scoring if meta-eval fails
   - Orchestrate all components together

3. Create `src/eval/runners/index.ts`:
   - Export AttemptRunner from attempt-runner.ts
   - Export EvalRunner from eval-runner.ts
   - Clean barrel exports

4. Add co-located tests in `src/eval/runners/__tests__/`:
   - `attempt-runner.test.ts` - Test 3-attempt loop with various failure scenarios
   - `eval-runner.test.ts` - Test full pipeline orchestration (mock dependencies)
   - Test edge cases (all failures, partial failures, API errors)
   - Test that all 3 attempts always complete
   - Aim for 80%+ coverage

**Acceptance Criteria**:
- [ ] AttemptRunner executes exactly 3 attempts per agent per fixture
- [ ] All 3 attempts complete even if some fail (no short-circuiting)
- [ ] Each attempt is independent (no retry logic)
- [ ] Errors are caught and categorized into failure outcomes
- [ ] EvalRunner orchestrates entire pipeline (fixtures → attempts → meta-eval → comparison)
- [ ] Meta-evaluation is called with all 3 attempts
- [ ] Results are stored via JSONReporter
- [ ] Reports are generated via MarkdownReporter
- [ ] Fallback scoring works if meta-eval fails
- [ ] Tests pass with 80%+ coverage (mocked dependencies)
- [ ] Linting passes: `bun run lint`
- [ ] Type checking passes: `bun run type-check`

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

- Error handling with try/catch per attempt (patterns.md)
- Layer boundaries respected (architecture.md):
  - Runners orchestrate, don't implement business logic
  - Delegate to evaluators and utilities
- No short-circuiting on failure (always complete all 3 attempts)

**TDD**: Follow `test-driven-development` skill
- Write tests for 3-attempt loop first
- Test various failure scenarios
- Watch tests fail
- Implement runners
- Watch tests pass

**Quality Gates**:
```bash
bun run lint
bun run type-check
bun test src/eval/runners/__tests__/
```

---

## Phase 4: Integration & Cleanup

**Strategy**: Sequential
**Reason**: Integration depends on all previous phases being complete.

### Task 6: Integration & Cleanup - Wire Everything Together

**Files**:
- `src/eval/run-eval.ts` (modified)
- `src/eval/index.ts` (modified)
- `src/eval/evaluator.ts` (deleted)
- `src/eval/runner.ts` (deleted)
- `src/eval/reporter.ts` (deleted)
- `src/eval/schemas.ts` (deleted)

**Complexity**: M (3h)

**Dependencies**: All previous tasks (wires everything together)

**Description**:
Integrate all new components, update entry points, and clean up old files. This completes the migration to the new multi-attempt evaluation system.

**Implementation Steps**:

1. Update `src/eval/run-eval.ts`:
   - Remove old imports (evaluator.ts, runner.ts, reporter.ts, schemas.ts)
   - Import new components:
     - EvalRunner from runners
     - MarkdownReporter, JSONReporter, CLIReporter from reporters
     - Schemas and types from core
   - Update main entry point to use EvalRunner
   - Wire up all reporters
   - Update CLI output for multi-attempt format
   - Ensure `bun run eval` command works

2. Update `src/eval/index.ts`:
   - Remove old exports
   - Export public API from new modules:
     - Core schemas and types
     - Runners (EvalRunner, AttemptRunner)
     - Evaluators (SingleAttemptEvaluator, MetaEvaluator)
     - Reporters (all three)
   - Keep public API clean (≤10 exports)
   - Add JSDoc for exported items

3. Delete old files:
   - `src/eval/evaluator.ts` (replaced by evaluators/single-attempt.ts)
   - `src/eval/runner.ts` (replaced by runners/eval-runner.ts)
   - `src/eval/reporter.ts` (split into reporters/)
   - `src/eval/schemas.ts` (moved to core/schemas.ts)
   - Remove any old tests for deleted files

4. Update fixtures if needed:
   - Ensure fixtures work with new multi-attempt system
   - Update fixture metadata if schema changed

5. Integration testing:
   - Run `bun run eval` end-to-end
   - Verify all 3 attempts execute
   - Check markdown report format
   - Check JSON files and symlinks
   - Verify CLI output shows per-attempt progress

**Acceptance Criteria**:
- [ ] `bun run eval` executes successfully
- [ ] All 3 attempts run for each agent on each fixture
- [ ] Markdown report shows per-attempt details with meta-evaluation
- [ ] JSON files created with timestamps and symlinks
- [ ] CLI output shows progress for each attempt
- [ ] Old files deleted (evaluator.ts, runner.ts, reporter.ts, schemas.ts)
- [ ] Public API exports are clean and documented
- [ ] No breaking changes to fixture format (or fixtures updated)
- [ ] All tests pass: `bun test`
- [ ] Linting passes: `bun run lint`
- [ ] Type checking passes: `bun run type-check`
- [ ] Coverage meets 80%+ target: `bun test --coverage`

**Mandatory Patterns**:

> **Constitution**: All code must follow @docs/constitutions/current/

- Clean public API (≤10 exports - architecture.md)
- Barrel exports (index.ts pattern)
- Integration testing before completion

**TDD**: Follow `test-driven-development` skill
- Write integration test first (end-to-end)
- Watch it fail
- Wire components together
- Watch test pass

**Quality Gates**:
```bash
bun run lint
bun run type-check
bun test
bun test --coverage
bun run eval  # End-to-end integration test
```

---

## Implementation Notes

### Constitution Compliance

All tasks MUST follow @docs/constitutions/current/ (v3):

- **architecture.md**: Layer boundaries, module organization, dependencies flow downward
- **patterns.md**: Error handling, validation, pure functions
- **schema-rules.md**: Schema-first development, validation at boundaries, type inference
- **tech-stack.md**: OpenAI SDK usage (`gpt-5`, `outputType`, `result.finalOutput`)
- **testing.md**: Co-located tests, 80%+ coverage, unit + integration tests

### OpenAI Agents SDK Pattern

All ChatGPT integration MUST use this pattern:

```typescript
import { Agent, run } from '@openai/agents';
import { z } from 'zod';

const schema = z.object({ score: z.number(), feedback: z.string() });

const agent = new Agent({
  name: 'Evaluator',
  instructions: 'Evaluate on scale 0-10...',
  model: 'gpt-5',  // Always gpt-5
  outputType: schema,  // NOT tools
});

const result = await run(agent, 'Your prompt here');
const output = result.finalOutput;  // NOT result.toolCalls
```

### Quality Standards

Every task must meet:

- ✅ Tests pass with 80%+ coverage
- ✅ Linting passes: `bun run lint`
- ✅ Type checking passes: `bun run type-check`
- ✅ No `any` types
- ✅ Explicit return types for public functions
- ✅ JSDoc comments for all exports

### Parallel Execution Strategy

**Phase 2 can be executed in parallel** using git-spice:

```bash
# From main branch
gs bc task-2-utility-layer
# Implement Task 2...

# Return to main
git checkout main
gs bc task-3-evaluator-layer
# Implement Task 3...

# Return to main
git checkout main
gs bc task-5-reporter-layer
# Implement Task 5...

# Submit all three PRs
gs stack submit
```

This achieves the 6h time savings by working on independent tasks simultaneously.

### Verification Checklist

Before marking implementation complete:

- [ ] All 6 tasks completed
- [ ] All tests pass: `bun test`
- [ ] Coverage ≥80%: `bun test --coverage`
- [ ] Linting passes: `bun run lint`
- [ ] Type checking passes: `bun run type-check`
- [ ] End-to-end: `bun run eval` executes successfully
- [ ] Markdown report shows per-attempt details
- [ ] JSON files created with correct format
- [ ] CLI output shows per-attempt progress
- [ ] No old files remain (evaluator.ts, runner.ts, reporter.ts, schemas.ts)
- [ ] Constitution compliance verified
